LDERY : YOU WILL NEED TO EITHER REINSTANTIATE THESE EXPERIMENTS OR TEST

CUDA_VISIBLE_DEVICES=0 python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/amazon/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 2000 --base_task_dataset_file datasets/amazon/train.jsonl --dev_task_file datasets/amazon/dev.jsonl --test_task_file datasets/amazon/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, -1.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --output_dir roberta-amazon-tapt_1.1.-1_repro_paper_config &> run_outputs/roberta-amazon-tapt_1.1.-1_repro_paper_config.txt



CUDA_VISIBLE_DEVICES=2 python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/amazon/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 500 --base_task_dataset_file datasets/amazon/train.jsonl --dev_task_file datasets/amazon/dev.jsonl --test_task_file datasets/amazon/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, 0.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --output_dir roberta-amazon-tapt_1.1.0_repro_paper_config --should_continue --classf_max_seq_len 384 &> run_outputs/roberta-amazon-tapt_1.1.0_repro_paper_config_continuing.txt



CUDA_VISIBLE_DEVICES=3 python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/amazon/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 500 --base_task_dataset_file datasets/amazon/train.jsonl --dev_task_file datasets/amazon/dev.jsonl --test_task_file datasets/amazon/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, -1.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --output_dir roberta-amazon-tapt_1.1.-1_repro_paper_config --should_continue --classf_max_seq_len 384 --classf_pretr_patience 50 &> run_outputs/roberta-amazon-tapt_1.1.-1_repro_paper_config_continuing_again.txt



cross experiments

CUDA_VISIBLE_DEVICES=0 python -u -m scripts.run_language_modeling --train_data_file datasets/imdb_data/all_training_joined.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/amazon/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 500 --base_task_dataset_file datasets/amazon/train.jsonl --dev_task_file datasets/amazon/dev.jsonl --test_task_file datasets/amazon/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, -1.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --output_dir transfer_imdb_amazon-tapt_1.1.-1_repro_paper_config --classf_max_seq_len 384 --classf_pretr_patience 50 &> run_outputs/roberta-amazon-tapt_1.1.-1_repro_paper_config_continuing_again.txt


CUDA_VISIBLE_DEVICES=1 python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file imdb_data/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 1000 --base_task_dataset_file datasets/imdb_data/train.jsonl --dev_task_file datasets/imdb_data/dev.jsonl --test_task_file datasets/imdb_data/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, -1.0)" --classifier_dropout 0.2 --output_dir transfer_amazon_imdb-tapt_1.1.-1_repro_paper_config --classf_lr 1e-4 --classf_max_seq_len 384 --classf_pretr_patience 50 --should_continue




CUDA_VISIBLE_DEVICES=1 python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/imdb_data/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 1000 --base_task_dataset_file datasets/imdb_data/train.jsonl --dev_task_file datasets/imdb_data/dev.jsonl --test_task_file datasets/imdb_data/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, -1.0)" --classifier_dropout 0.2 --output_dir roberta-transfer_amazon_imdb-tapt_1.1.-1_repro_paper_config --classf_lr 1e-4 --classf_max_seq_len 384 --classf_pretr_patience 50 --should_continue &> run_outputs/transfer_amazon_to_imdb-tapt_1.1.-1_repro_paper_config_continuing_again.txt