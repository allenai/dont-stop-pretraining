LDERY : YOU WILL NEED TO EITHER REINSTANTIATE THESE EXPERIMENTS OR TEST

CUDA_VISIBLE_DEVICES=0 python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/amazon/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 2000 --base_task_dataset_file datasets/amazon/train.jsonl --dev_task_file datasets/amazon/dev.jsonl --test_task_file datasets/amazon/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, -1.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --output_dir roberta-amazon-tapt_1.1.-1_repro_paper_config &> run_outputs/roberta-amazon-tapt_1.1.-1_repro_paper_config.txt



CUDA_VISIBLE_DEVICES=2 python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/amazon/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 500 --base_task_dataset_file datasets/amazon/train.jsonl --dev_task_file datasets/amazon/dev.jsonl --test_task_file datasets/amazon/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, 0.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --output_dir roberta-amazon-tapt_1.1.0_repro_paper_config --should_continue --classf_max_seq_len 384 &> run_outputs/roberta-amazon-tapt_1.1.0_repro_paper_config_continuing.txt



CUDA_VISIBLE_DEVICES=3 python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/amazon/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 500 --base_task_dataset_file datasets/amazon/train.jsonl --dev_task_file datasets/amazon/dev.jsonl --test_task_file datasets/amazon/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, -1.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --output_dir roberta-amazon-tapt_1.1.-1_repro_paper_config --should_continue --classf_max_seq_len 384 --classf_pretr_patience 50 &> run_outputs/roberta-amazon-tapt_1.1.-1_repro_paper_config_continuing_again.txt



cross experiments

CUDA_VISIBLE_DEVICES=0 python -u -m scripts.run_language_modeling --train_data_file datasets/imdb_data/all_training_joined.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/amazon/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 500 --base_task_dataset_file datasets/amazon/train.jsonl --dev_task_file datasets/amazon/dev.jsonl --test_task_file datasets/amazon/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, -1.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --output_dir transfer_imdb_amazon-tapt_1.1.-1_repro_paper_config --classf_max_seq_len 384 --classf_pretr_patience 50 &> run_outputs/roberta-amazon-tapt_1.1.-1_repro_paper_config_continuing_again.txt


CUDA_VISIBLE_DEVICES=1 python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file imdb_data/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 1000 --base_task_dataset_file datasets/imdb_data/train.jsonl --dev_task_file datasets/imdb_data/dev.jsonl --test_task_file datasets/imdb_data/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, -1.0)" --classifier_dropout 0.2 --output_dir transfer_amazon_imdb-tapt_1.1.-1_repro_paper_config --classf_lr 1e-4 --classf_max_seq_len 384 --classf_pretr_patience 50 --should_continue




CUDA_VISIBLE_DEVICES=1 python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/imdb_data/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 100 --learning_rate 0.0005 --logging_steps 1000 --base_task_dataset_file datasets/imdb_data/train.jsonl --dev_task_file datasets/imdb_data/dev.jsonl --test_task_file datasets/imdb_data/test.jsonl --num_samples_for_basis 10  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -6 --eta_set "(1.0, 1.0, -1.0)" --classifier_dropout 0.2 --output_dir roberta-transfer_amazon_imdb-tapt_1.1.-1_repro_paper_config --classf_lr 1e-4 --classf_max_seq_len 384 --classf_pretr_patience 50 --should_continue &> run_outputs/transfer_amazon_to_imdb-tapt_1.1.-1_repro_paper_config_continuing_again.txt




TIR CLUSTER RUN SETUP


lm_mt_task_weight selected based on relative dataset sizes

For Amazon dataset
python -u -m scripts.run_language_modeling --train_data_file datasets/amz_w_imdb_joined.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 16 --gradient_accumulation_steps 128 --model_name_or_path roberta-base --eval_data_file datasets/amazon/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 50 --learning_rate 0.0005 --logging_steps 2000 --base_task_dataset_file datasets/amazon/train.jsonl --dev_task_file datasets/amazon/dev.jsonl --test_task_file datasets/amazon/test.jsonl --num_samples_for_basis 32  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -12 --eta_set "(1.0, 1.0, 0.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --lm_mt_task_weight 0.1 --output_dir output_models/roberta-amazon-tapt_1.1.0_vwgt_0.1_joined_amz_imdb_data

For imdb
python -u -m scripts.run_language_modeling --train_data_file datasets/amz_w_imdb_joined.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 16 --gradient_accumulation_steps 128 --model_name_or_path roberta-base --eval_data_file datasets/imdb_data/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 50 --learning_rate 0.0005 --logging_steps 2000 --base_task_dataset_file datasets/imdb_data/train.jsonl --dev_task_file datasets/imdb_data/dev.jsonl --test_task_file datasets/imdb_data/test.jsonl --num_samples_for_basis 32  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -12 --eta_set "(1.0, 1.0, 0.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --lm_mt_task_weight 0.02 --output_dir output_models/roberta-imdb-tapt_1.1.0_vwgt_0.1_joined_amz_imdb_data



GCLOUD RUN SETUP

mkdir -p output_models
mkdir -p dsp_logs

Direct experiments : Amazon
python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/amazon/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 50 --learning_rate 0.0005 --logging_steps 2000 --base_task_dataset_file datasets/amazon/train.jsonl --dev_task_file datasets/amazon/dev.jsonl --test_task_file datasets/amazon/test.jsonl --num_samples_for_basis 32  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -12 --eta_set "(1.0, 1.0, 0.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --lm_mt_task_weight 0.1 --output_dir output_models/roberta-amazon-tapt_1.1.0_vwgt_0.1_direct &> dsp_logs/roberta-amazon-tapt_1.1.0_vwgt_0.1_direct.txt

Direct Experiments : Imdb
python -u -m scripts.run_language_modeling --train_data_file datasets/imdb_data/all_training_joined.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 16 --gradient_accumulation_steps 128 --model_name_or_path roberta-base --eval_data_file datasets/imdb_data/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 50 --learning_rate 0.0005 --logging_steps 2000 --base_task_dataset_file datasets/imdb_data/train.jsonl --dev_task_file datasets/imdb_data/dev.jsonl --test_task_file datasets/imdb_data/test.jsonl --num_samples_for_basis 32  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -12 --eta_set "(1.0, 1.0, 0.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --lm_mt_task_weight 0.02 --output_dir output_models/roberta-imdb-tapt_1.1.0_vwgt_0.02_direct &> dsp_logs/roberta-imdb-tapt_1.1.0_vwgt_0.02_direct.txt

Indirect : Amazon to Imdb
python -u -m scripts.run_language_modeling --train_data_file datasets/amazon/train.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 16 --gradient_accumulation_steps 128 --model_name_or_path roberta-base --eval_data_file datasets/imdb_data/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 50 --learning_rate 0.0005 --logging_steps 2000 --base_task_dataset_file datasets/imdb_data/train.jsonl --dev_task_file datasets/imdb_data/dev.jsonl --test_task_file datasets/imdb_data/test.jsonl --num_samples_for_basis 32  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -12 --eta_set "(1.0, 1.0, 0.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --lm_mt_task_weight 0.02 --output_dir output_models/roberta-imdb-tapt_1.1.0_vwgt_0.02_indirect_amazn_to_imdb &> dsp_logs/roberta-imdb-tapt_1.1.0_vwgt_0.02_indirect_amazn_to_imdb.txt

Indirect : Imdb to Amazon
python -u -m scripts.run_language_modeling --train_data_file datasets/imdb_data/all_training_joined.txt --line_by_line  --model_type roberta-base --tokenizer_name roberta-base --mlm --per_gpu_train_batch_size 4 --gradient_accumulation_steps 512 --model_name_or_path roberta-base --eval_data_file datasets/amazon/dev.txt  --do_eval  --evaluate_during_training  --do_train --num_train_epochs 50 --learning_rate 0.0005 --logging_steps 2000 --base_task_dataset_file datasets/amazon/train.jsonl --dev_task_file datasets/amazon/dev.jsonl --test_task_file datasets/amazon/test.jsonl --num_samples_for_basis 32  --num_basis 5 --overwrite_output_dir --pca_every 1 --n_subspace_layers -12 --eta_set "(1.0, 1.0, 0.0)" --classifier_dropout 0.2 --classf_lr 1e-4 --lm_mt_task_weight 0.1 --output_dir output_models/roberta-amazon-tapt_1.1.0_vwgt_0.1_indirect_imdb_to_amazn &> dsp_logs/roberta-amazon-tapt_1.1.0_vwgt_0.1_indirect_imdb_to_amazn.txt









